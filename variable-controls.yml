 ##FACTORS UNDER EXPERIMENT
 
  ###FACTOR A###
  #You can adjust the net_rx_action budget, which determines how much packet processing can be spent
  #among all NAPI structures registered to a CPU by setting a sysctl value named net.core.netdev_budget.
- name: (Factor A) Increase NAPI Budget to {{NAPI_budget[1]|int*test_counter|int}}
  shell: sysctl -w net.core.netdev_budget={{NAPI_budget[1]|int*test_counter|int}}
  become: yes
  when: "'A' in current_factor_list"
  
  ###FACTOR B###
  #https://community.mellanox.com/s/article/linux-sysctl-tuning
  #https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/
- name: (Factor B) Set Kernel Max Backlog to {{backlog[1]|int*test_counter|int}}
  shell: sysctl -w net.core.netdev_max_backlog={{backlog[1]|int*test_counter|int}}
  become: yes
  when: "'B' in current_factor_list"

  ###FACTOR C###
  #https://www.kernel.org/doc/Documentation/networking/scaling.txt
  #http://christophe.vandeplas.com/2013/11/suricata-capturekerneldrops-caused-by.html
  #Has some bugs. Probably a better way. Doesnt seem to apply much to ARM boards
#- name: Determine CPU serving NIC Interrupts
#  shell: cat /proc/interrupts | grep -oP "\d+:\s+\K(.+)(?={{capture_interface}})" | tr -s [:space:] "\n" | head -$(grep -c ^processor /proc/cpuinfo) | cat -n | sort -k2rn | head -1 | cut -b5-6
#  register: irq_affinity
#  changed_when: false
#  when: "'C' in current_factor_list" 

- name: (Factor C) Enable / Set Receive Packet Steering Affinity with Mask {{ rps_mask[1] }}
  #shell: "echo {{ bitmasks[irq_affinity.stdout_lines|int] }} > /sys/class/net/{{capture_interface}}/queues/rx-0/rps_cpus"
  #All the ARM boards tested so far use CPU0 for interrupts (the cpu that booted the kernel)
  shell: "echo {{ rps_mask[1] }} > /sys/class/net/{{capture_interface}}/queues/rx-0/rps_cpus"
  become: yes
  ignore_errors: yes
  when: "'C' in current_factor_list"


  ###FACTOR D###
  #https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data
  #Receive flow steering (RFS) is used in conjunction with RPS. RPS attempts to distribute incoming packet load amongst multiple CPUs,
  #but does not take into account any data locality issues for maximizing CPU cache hit rates.
  #You can use RFS to help increase cache hit rates by directing packets for the same flow to the same CPU for processing.

  ##RFS keeps track of a global hash table of all flows and the size of this hash table can be adjusted by setting the net.core.rps_sock_flow_entries sysctl.
  ##You can also set the number of flows per RX queue by writing this value to the sysfs file named rps_flow_cnt for each RX queue.
- name: (Factor D) Set Receive Flow Steering (RFS) Table Size to {{rfs_table[1]|int*test_counter|int}} 
  shell: |
    sysctl -w net.core.rps_sock_flow_entries="{{ rfs_table[1]|int*test_counter|int }}"
    echo "{{ rfs_flow_cnt }}" > /sys/class/net/{{capture_interface}}/queues/rx-0/rps_flow_cnt
  become: yes
  ignore_errors: yes
  when: "'D' in current_factor_list"

  ###FACTOR E###
  #This appears to be set very low by default for legacy compatibility?
  #Can set them larger since we dont care as much about latency
  #rpi drivers dont support this so they are only 4 factors
- name: (Factor E) Set Rx-Ring Size to Pre-set Maximums
  shell: "ethtool -G {{capture_interface}} rx $(ethtool -g {{capture_interface}} | grep RX: | head -1 | awk '{print $2}')"
  become: yes
  ignore_errors: yes
  register: ring_result
  failed_when: "'Operation not supported' in ring_result.stderr"
  when: "'E' in current_factor_list and 'rpi' not in group_names"

